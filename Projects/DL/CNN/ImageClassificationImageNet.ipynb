{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImageNet dataset has more than 14 million images, hand-labeled across 20,000 categories\n",
    "\n",
    "The downside – is that its too much for an everyday laptop. So the alternative solution: That’s where Fast.ai‘s Imagenette dataset comes in.\n",
    "> Download this to the current folder: https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_map = { \n",
    "    \"n01440764\" : \"tench\",\n",
    "    \"n02102040\" : \"springer\",\n",
    "    \"n02979186\" : \"casette_player\",\n",
    "    \"n03000684\" : \"chain_saw\",\n",
    "    \"n03028079\" : \"church\",\n",
    "    \"n03394916\" : \"French_horn\",\n",
    "    \"n03417042\" : \"garbage_truck\",\n",
    "    \"n03425413\" : \"gas_pump\",\n",
    "    \"n03445777\" : \"golf_ball\",\n",
    "    \"n03888257\" : \"parachute\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagegen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9469 images belonging to 10 classes.\n",
      "Found 3925 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train = imagegen.flow_from_directory(\"imagenette2/train/\", class_mode = \"categorical\", shuffle = False, batch_size = 128, target_size = (224, 224))\n",
    "val = imagegen.flow_from_directory(\"imagenette2/val/\", class_mode = \"categorical\", shuffle = False, batch_size = 128, target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNN model for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(InputLayer(input_shape = (224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st convolutional block\n",
    "model.add(Conv2D(25, (5, 5), activation = 'relu', strides = (1, 1), padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), padding = 'same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd convolutional block\n",
    "model.add(Conv2D(50, (5, 5), activation = 'relu', strides = (2, 2), padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), padding = 'same'))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd convolutional block\n",
    "model.add(Conv2D(70, (3, 3), activation = 'relu', strides = (2, 2), padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), padding = 'valid'))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN (Artificial Neural Networks) block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 100, activation = 'relu'))\n",
    "model.add(Dense(units = 100, activation = 'relu'))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "model.add(Dense(units = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = \"adam\")\n",
    "# deep architecture than what we have utilized so far, we are only able to get a validation accuracy of around 40-50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lidor\\AppData\\Local\\Temp\\ipykernel_4780\\2699342898.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train, epochs = N, validation_data = val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "74/74 [==============================] - 234s 3s/step - loss: 2.5344 - accuracy: 0.1393 - val_loss: 3.0824 - val_accuracy: 0.1141\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 223s 3s/step - loss: 2.1076 - accuracy: 0.2675 - val_loss: 2.1511 - val_accuracy: 0.2739\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 266s 4s/step - loss: 1.8631 - accuracy: 0.3665 - val_loss: 1.8768 - val_accuracy: 0.3625\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 240s 3s/step - loss: 1.5537 - accuracy: 0.4800 - val_loss: 1.9371 - val_accuracy: 0.3564\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 227s 3s/step - loss: 1.3432 - accuracy: 0.5519 - val_loss: 3.4085 - val_accuracy: 0.1391\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 226s 3s/step - loss: 1.1910 - accuracy: 0.5956 - val_loss: 2.1495 - val_accuracy: 0.3518\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 229s 3s/step - loss: 0.9949 - accuracy: 0.6626 - val_loss: 2.1247 - val_accuracy: 0.3625\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 228s 3s/step - loss: 0.8186 - accuracy: 0.7211 - val_loss: 1.8492 - val_accuracy: 0.4181\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 230s 3s/step - loss: 0.7295 - accuracy: 0.7600 - val_loss: 2.1040 - val_accuracy: 0.4209\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 224s 3s/step - loss: 0.6241 - accuracy: 0.7974 - val_loss: 1.9103 - val_accuracy: 0.4408\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 225s 3s/step - loss: 0.4332 - accuracy: 0.8613 - val_loss: 2.1224 - val_accuracy: 0.3906\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 223s 3s/step - loss: 0.2763 - accuracy: 0.9168 - val_loss: 1.9811 - val_accuracy: 0.4581\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 223s 3s/step - loss: 0.1938 - accuracy: 0.9417 - val_loss: 2.3531 - val_accuracy: 0.4313\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 224s 3s/step - loss: 0.1738 - accuracy: 0.9453 - val_loss: 2.5389 - val_accuracy: 0.4054\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 224s 3s/step - loss: 0.1422 - accuracy: 0.9553 - val_loss: 2.2858 - val_accuracy: 0.4538\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 223s 3s/step - loss: 0.0926 - accuracy: 0.9753 - val_loss: 2.5575 - val_accuracy: 0.4155\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 223s 3s/step - loss: 0.0666 - accuracy: 0.9818 - val_loss: 2.9158 - val_accuracy: 0.4239\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 217s 3s/step - loss: 0.0427 - accuracy: 0.9905 - val_loss: 2.8189 - val_accuracy: 0.4268\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 222s 3s/step - loss: 0.0324 - accuracy: 0.9918 - val_loss: 2.6114 - val_accuracy: 0.4375\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 224s 3s/step - loss: 0.0319 - accuracy: 0.9931 - val_loss: 2.8937 - val_accuracy: 0.4346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bddc54c0d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit for N epochs\n",
    "N = 20\n",
    "model.fit_generator(train, epochs = N, validation_data = val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning (VGG16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 3s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = VGG16(include_top = False, weights = 'imagenet')\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1249s 17s/step\n",
      "31/31 [==============================] - 519s 17s/step\n"
     ]
    }
   ],
   "source": [
    "vgg_features_train = pretrained_model.predict(train)\n",
    "vgg_features_val = pretrained_model.predict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = to_categorical(train.labels)\n",
    "val_target = to_categorical(val.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Flatten(input_shape=(7, 7, 512)))\n",
    "model2.add(Dense(100, activation = 'relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               2508900   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2510310 (9.58 MB)\n",
      "Trainable params: 2510110 (9.58 MB)\n",
      "Non-trainable params: 200 (800.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "74/74 [==============================] - 3s 41ms/step - loss: 0.0920 - accuracy: 0.9837 - val_loss: 0.1782 - val_accuracy: 0.9450\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0475 - accuracy: 0.9948 - val_loss: 0.1731 - val_accuracy: 0.9460\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0273 - accuracy: 0.9979 - val_loss: 0.1721 - val_accuracy: 0.9457\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 0.0193 - accuracy: 0.9990 - val_loss: 0.1746 - val_accuracy: 0.9462\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0138 - accuracy: 0.9996 - val_loss: 0.1789 - val_accuracy: 0.9460\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0130 - accuracy: 0.9993 - val_loss: 0.1762 - val_accuracy: 0.9450\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0093 - accuracy: 0.9996 - val_loss: 0.1792 - val_accuracy: 0.9457\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0078 - accuracy: 0.9996 - val_loss: 0.1841 - val_accuracy: 0.9439\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0070 - accuracy: 0.9996 - val_loss: 0.1911 - val_accuracy: 0.9417\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0066 - accuracy: 0.9996 - val_loss: 0.1987 - val_accuracy: 0.9424\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.0058 - accuracy: 0.9996 - val_loss: 0.1906 - val_accuracy: 0.9437\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.0045 - accuracy: 0.9999 - val_loss: 0.1890 - val_accuracy: 0.9462\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.0046 - accuracy: 0.9999 - val_loss: 0.1910 - val_accuracy: 0.9450\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 3s 38ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.1979 - val_accuracy: 0.9455\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.2007 - val_accuracy: 0.9447\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.2166 - val_accuracy: 0.9404\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 0.2032 - val_accuracy: 0.9452\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9434\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.2148 - val_accuracy: 0.9439\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.2130 - val_accuracy: 0.9445\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 3s 38ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.2179 - val_accuracy: 0.9417\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 3s 38ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 0.2111 - val_accuracy: 0.9422\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 3s 38ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.2116 - val_accuracy: 0.9401\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.2110 - val_accuracy: 0.9427\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 0.0036 - accuracy: 0.9996 - val_loss: 0.2208 - val_accuracy: 0.9414\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.2341 - val_accuracy: 0.9411\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 3s 40ms/step - loss: 0.0026 - accuracy: 0.9999 - val_loss: 0.2202 - val_accuracy: 0.9432\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.2420 - val_accuracy: 0.9366\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.2425 - val_accuracy: 0.9389\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.2594 - val_accuracy: 0.9371\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0066 - accuracy: 0.9984 - val_loss: 0.2556 - val_accuracy: 0.9383\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.2445 - val_accuracy: 0.9399\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.2373 - val_accuracy: 0.9383\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.2952 - val_accuracy: 0.9302\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.2380 - val_accuracy: 0.9391\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.2502 - val_accuracy: 0.9404\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.2459 - val_accuracy: 0.9383\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.2573 - val_accuracy: 0.9378\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.2539 - val_accuracy: 0.9391\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.2431 - val_accuracy: 0.9396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bde5b12460>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 40\n",
    "model2.fit(vgg_features_train, train_target, epochs = N, batch_size = 128, validation_data = (vgg_features_val, val_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future steps, need to try the same on the following datasubsets\n",
    " - Imagewoof: 10 classes of dog breeds – a larger batch size and a more difficult problem to classify.\n",
    " - Image网 (“wang”): A combination of Imagenette and Imagewoof and a couple of tricks that make it a harder problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
